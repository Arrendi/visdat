---
title: "Using visdat"
author: "Nicholas Tierney"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}

knitr::opts_chunk$set(fig.width = 5,
                      fig.height = 4)

```


In his [R for Data Science book](http://r4ds.had.co.nz/), Wickham describes 6 phases of data science:

![Process of Data Science](http://r4ds.had.co.nz/diagrams/data-science.png)

You can get insight into your data by modelling, visualising, and transforming, which Wickham describes as "Understanding" or "knowledge generation". There is some overlap here, as in this process it wouldn't be surprising that you might uncover some feature of your dataset that you would need to clean up - you might discover that some strings were indeed factors, or that gender was considered numeric in your regression, warranting you to re-visit the Tidying phase.

In the same way, I think that this process gets applied in the Tidying step of data science. You read in your data, but you then need to look at it to understand what you need to do to "fix it". And that's a key phrase here "looking at the data" - what does that mean?

On one hand, you can look at the head of the data:

```{r head-iris}

head(iris)

```

Or you can have a `glimpse` at it through `dplyr::glimpse`

```{r glimpse}

dplyr::glimpse(iris)

```

Here we see that we have doubles, and a factor. We get some insight into the data.

But we don't always have data like the canonical iris dataset. let's take a look at some data that might be a bit more typical of "messy" data.

```{r visdat-glimpse}
library(visdat)
dplyr::glimpse(typical_data)

```

Looking at this, you might then ask:

> Isn't it odd that Income is a factor? And Age is a character? 

And you might start to wonder what else is different, what else changed? 

And it might be a bit unclear where to go from there. Do you plot the data? Why does my plot look wierd? What are these other strange features in the data? The `visdat` package provides visualisations of an entire dataframe at once. Initially inspired by [`csv-fingerprint`](https://github.com/setosa/csv-fingerprint), `visdat` provides tools to create heatmap-like visualisations of an entire dataframe. `visdat` provides 2 main functions `vis_dat` and `vis_miss`.

`vis_dat()` helps explore the data class structure and missingness:

```{r load-data}

vis_dat(typical_data)

```

And the `vis_miss` function provides a custom plot for missing data.

```{r}

vis_miss(typical_data)

```

The name `visdat` was chosen as it borrows from the idea of [`testdat`](https://github.com/ropensci/testdat), which provides unit testing for your data.  In a similar way, `visdat` provides visual tests, the idea being that first you visualise your data (`visdat`), then you run tests from `testdat`, or a package like `assertr`, to fix these errors.

## Using `vis_dat`

Let's see what's inside the dataset `airquality`

```{r vis_dat}

library(visdat)

vis_dat(airquality)

```

The plot above tells us that R reads this dataset as having numeric and integer values, along with some missing data in `Ozone` and `Solar.R`. The classes are represented on the legend, and missing data represented by black. 

By default, `vis_dat` sorts the columns according to the type of the data in the vectors. You can turn this off by setting `sort_type == FALSE`. This feature is better illustrated using the `typical_data` dataset, created using [wakefield](github.com/trinker/wakefield) and contained within visdat

```{r visdat-typical}

vis_dat(typical_data)

vis_dat(typical_data, 
        sort_type = FALSE)

```

## using `vis_miss`

We can explore the missing data further using `vis_miss`

```{r vis_miss}

vis_miss(airquality)

```

Notice that the percentages of missingness are provided in the data. These are accurate to 1 decimal place. When there is <0.1% of missingness, `vis_miss` indicates that there is >1% missingness.

```{r vismiss-new-data}

df_test <- data.frame(x1 = 1:10000,
                      x2 = rep("A", 10000),
                      x3 = c(rep(1L, 9999), NA))

vis_miss(df_test)

```

`vis_miss` will also indicate when there is no missing data at all. 

```{r vismiss-mtcars}

df_test <- data.frame(x1 = 1:10000,
                      x2 = rep("tidy", 10000),
                      x3 = rep("data", 10000))

vis_miss(df_test)

```

Columns can be arranged by columns with most missingness, by setting `sort_miss = TRUE`.

```{r vismiss}

vis_miss(airquality,
         sort_miss = TRUE)

```

And missingness can be clustered by setting `cluster = TRUE`

```{r vis_miss-cluster}

vis_miss(airquality, 
         cluster = TRUE)

```

To further explore the missingness structure in a dataset, I recommend the [`naniar`](https://github.com/njtierney/naniar) package, which provides more general tools for graphical and numerical exploration of missing values.

## Interactivity

Thanks to Carson Sievert, you can now add some really nifty interactivity into visdat by using `plotly::ggplotly`, allowing for information to be revealed upon mouseover of a cell.

```{r plotly-example}

library(plotly)

vis_dat(airquality, flip = TRUE) %>% ggplotly()
vis_miss(airquality) %>% ggplotly()

```

There is also an in-development feature, `vis_miss_ly`, which directly uses plotly's code, which is faster than calling ggplotly(vis_dat(df)).

```{r}

vis_miss_ly(airquality)

```

Although some work is needed still to fix the legend and fill, and add percentages to the legend.

## using `vis_guess`

`vis_guess` takes a guess at what each cell is. It's best illustrated using some messy data, which we'll make here.

```{r create-messy-vector}

messy_vector <- c(TRUE,
                  T,
                  "TRUE",
                  "T",
                  "01/01/01",
                  "01/01/2001",
                  NA,
                  NaN,
                  "NA",
                  "Na",
                  "na",
                  "10",
                  10,
                  "10.1",
                  10.1,
                  "abc",
                  "$%TG")

messy_df <- data.frame(var1 = messy_vector,
                       var2 = sample(messy_vector),
                       var3 = sample(messy_vector))

```

Here if we just glimpse `messy_df`, we get 

```{r}

dplyr::glimpse(messy_df)

```

It looks like we have 3 factors. But this seems a bit strange. `vis_guess` takes a guess at what each element is, visualising it for you:

```{r messy-vis-guess}

vis_guess(messy_df)

```

So here we see that there are many different kinds of data in your dataframe. As an analyst this might be a depressing finding. Compare this to `vis_dat`.

```{r visdat-messy}

vis_dat(messy_df)

```

Where you'd just assume your data is wierd because it's all factors - or worse, not notice that this is a problem.

At the moment `vis_guess` is slow. Please take this into consideration when you are using it on data with more than 1000 rows. We're looking into ways of making it faster, potentially using methods from the `parallel` package, or extending the c++ code from `readr::parse_guess`.


# Future work and experimental features

**visualising expectations**

The idea is to pass expectations into `vis_dat` or `vis_miss`, along the lines of the `expectation` command in `assertr`. For example, you could ask `vis_dat` to identify those cells with values of -1 with something like this:

```{r eval = FALSE}

data %>% 
  expect(value == -1) %>%
  vis_dat

```

